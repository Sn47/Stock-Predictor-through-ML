# -*- coding: utf-8 -*-
"""stockpredictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GMcKa0muAi3n3CHqT3SdgIv_nxSCToHk

# **Importing required libraries**
"""

#Importing required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#!pip install --upgrade tensorflow
#!pip install --upgrade keras

!pip install keras==2.12.0
!pip install tensorflow==2.12.0

import tensorflow as tf
print("TensorFlow version:", tf.__version__)

import keras
print("Keras version:", keras.__version__)

"""# Loading the data"""

#Loading the data
df = pd.read_csv('/S&P.csv')
new_d=df.copy()
df = df.iloc[::-1]

df = df.reset_index(drop=True)
print(df)

df.info()
df.describe()
df.isnull().sum()

import datetime

def str_to_datetime(s):
  split = s.split('-')
  year, month, day = int(split[0]), int(split[1]), int(split[2])
  return datetime.datetime(year=year, month=month, day=day)

datetime_object = str_to_datetime('1986-03-19')
datetime_object

# Define the function df_to_windowed_df here
def df_to_windowed_df(dataframe, first_date_str, last_date_str, n=3):
    first_date = pd.to_datetime(first_date_str)
    last_date = pd.to_datetime(last_date_str)

    target_date = first_date

    dates = []
    X, Y, volumes = [], [], []

    while target_date <= last_date:
        if target_date in dataframe.index:
            df_subset = dataframe.loc[:target_date].tail(n+1)

            if len(df_subset) == n+1:
                values = df_subset[['Close', 'Volume']].to_numpy()
                x, y, volume = values[:-1, 0], values[-1, 0], values[-1, 1]

                dates.append(target_date)
                X.append(x)
                Y.append(y)
                volumes.append(volume)

        # Move to the next date
        target_date += datetime.timedelta(days=1)

    ret_df = pd.DataFrame({})
    ret_df['Target Date'] = dates

    X = np.array(X)
    for i in range(0, n):
        ret_df[f'Target-{n-i}'] = X[:, i]

    ret_df['Volume'] = volumes
    ret_df['Target'] = Y

    return ret_df


# Convert 'Date' to datetime and set as index
df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')
data = df.set_index('Date')

# Use the df_to_windowed_df function after preparing the data
windowed_df = df_to_windowed_df(data, '2002-02-21', '2012-10-23', n=3)

print(windowed_df)

"""# MODEL 01

**Scalling**
"""

from sklearn.preprocessing import MinMaxScaler


columns_to_scale = ['Target-3', 'Target-2', 'Target-1', 'Target','Volume']

# Create a MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# Scale only the numerical columns
numerical_data = windowed_df[columns_to_scale]
scaled_data = scaler.fit_transform(numerical_data)

windowed_df[columns_to_scale] = scaled_data

print(windowed_df)

#windowed_df.index = windowed_df.pop('Target Date')
#windowed_df

"""Target natural graph"""

plt.plot(windowed_df['Target Date'],windowed_df['Target'])

"""Extracting "Date" , "Inputs(features)" and , "Output(target)"
"""

# Convert the DataFrame to a NumPy array
df_as_np = windowed_df.to_numpy()



# Extract dates from the first column
dates = df_as_np[:,0]


# Extract the feature matrix from columns 1 to second-to-last column
middle_matrix = df_as_np[:, 1:-1]

# Reshape the feature matrix
X = middle_matrix.reshape((len(dates), middle_matrix.shape[1], 1))

# Extract the target values from the last column
Y = df_as_np[:, -1]


X = X.astype(np.float32)
y = Y.astype(np.float32)

# Print the shapes of the arrays
print("Dates shape:", dates.shape)
print("X shape:", X.shape)
print("Y shape:", Y.shape)

"""Splitting into train , test and validator"""

q_80 = int(len(dates)* .8)
q_90 = int(len(dates)* .9)

dates_train, X_train, y_train = dates[:q_80],X[:q_80],y[:q_80]


dates_val, X_val, y_val = dates[q_80:q_90],X[q_80:q_90],y[q_80:q_90]
dates_test, X_test, y_test = dates[q_90:],X[q_90:],y[q_90:]

plt.plot(dates_train, y_train)
plt.plot(dates_val, y_val)
plt.plot(dates_test, y_test)

plt.legend(['Train', 'Validation', 'Test'])

""" **GRIDSEARCH and KERAS Classifier To find the optimum number of hidden layers**"""

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from keras.layers import Dense, Activation
from keras.activations import relu,sigmoid

from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers

def create_model(layers,activation):
  m=Sequential()
  for i,nodes in enumerate(layers):
    if i==0:
      m.add(Dense(nodes,input_dim=X_train.shape[1]))
      m.add(Activation(activation))
    else:
      m.add(Dense(nodes))
      m.add(Activation(activation))
    m.add(Dense(1))
    m.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
    return m

m=KerasClassifier(build_fn=create_model,verbose=0)

m

"""Gridsearch"""

layers=[[20],[40,20],[45,30,15],[64,32,16],[64,32]]
activitations=['sigmoid','relu']
param_grid=dict(layers=layers,activation=activitations,batch_size=[128,256],epochs=[30])
grid=GridSearchCV(estimator=m,param_grid=param_grid)

#grid_result=grid.fit(X_train,y_train)

#[grid_result.best_score_,grid_result.best_params_]

"""# TRAINING THE FIRST MODEL"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers


model = Sequential([layers.Input((4, 1)),
                    layers.LSTM(40),
                    layers.Dense(20, activation='sigmoid'),
                    layers.Dense(1)])

model.compile(loss='mse',
              optimizer=Adam(learning_rate=0.001),
              metrics=['mean_absolute_error'])

model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30,batch_size=128)

#predicting values for training data
train_predictions = model.predict(X_train).flatten()

plt.plot(dates_train, train_predictions)
plt.plot(dates_train, y_train)
plt.legend(['Training Predictions', 'Training Observations'])

#predicting values for validation data
val_predictions = model.predict(X_val).flatten()
plt.plot(dates_val, val_predictions)
plt.plot(dates_val, y_val)
plt.legend(['Validation Predictions', 'Validation Observations'])

#prdeicting values for testing
test_predictions = model.predict(X_test).flatten()
plt.plot(dates_test, test_predictions)
plt.plot(dates_test, y_test)
plt.legend(['Testing Predictions', 'Testing Observations'])

"""**EVALUATION METRICS**

RMSE
"""

import numpy as np
from sklearn.metrics import mean_squared_error
rmse = np.sqrt(mean_squared_error(y_test, test_predictions))
print(rmse)

"""MAE"""

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, test_predictions)
print(mae)

"""MSE"""

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, test_predictions)
print(mse)

"""R2"""

from sklearn.metrics import r2_score
r2 = r2_score(y_test, test_predictions)
print(r2)

"""MAPE"""

def mean_absolute_percentage_error(y_true, y_pred):
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape = mean_absolute_percentage_error(y_test, test_predictions)
print(mape)

"""The values of evaluation metrics may same perfect but our model have some fundamental issues as we have implemented the basic features using simplest of features

PREDICTING FUTURE VALUES
"""

from copy import deepcopy
#test_predictions = model.predict(X_test).flatten()
recursive_predictions = []
#recursive_dates = np.concatenate([dates_val, dates_test])
recursive_dates = dates_val.copy()

last_window = deepcopy(X_train[-1])  # Start with the last window from the training set
#test_predictions = model.predict(X_test).flatten()
print(last_window)
for target_date in recursive_dates:
#    # Predict the next value based on the current window
    next_prediction = model.predict(np.array([last_window])).flatten()
    recursive_predictions.append(next_prediction)
    print(next_prediction)

    last_window = np.roll(last_window, -1, axis=0)
    last_window[-1] = next_prediction

import matplotlib.pyplot as plt

# Define colors for each line
colors = {
    'Training Predictions': 'blue',
    'Training Observations': 'green',
    'Validation Predictions': 'orange',
    'Validation Observations': 'red',
    'Testing Predictions': 'purple',
    'Testing Observations': 'brown',
    'Recursive Predictions': 'magenta'
}

# Plot the data with labels and custom styles
plt.figure(figsize=(12, 6))  # Adjust the figure size

for label in colors.keys():
    plt.plot([], [], label=label, color=colors[label])  # Create empty lines for legend
n=50  #window size for prediction
# Plot the actual data

#plt.plot(dates_train, train_predictions, color=colors['Training Predictions'])
#plt.plot(dates_train, y_train, linestyle='--', color=colors['Training Observations'])
plt.plot(dates_val[:n], val_predictions[:n], color=colors['Validation Predictions'])
plt.plot(dates_val[:n], y_val[:n], linestyle='--', color=colors['Validation Observations'])
#plt.plot(dates_test, test_predictions, color=colors['Testing Predictions'])
#plt.plot(dates_test, y_test, linestyle='--', color=colors['Testing Observations'])
plt.plot(recursive_dates[:n], recursive_predictions[:n], color=colors['Recursive Predictions'])

# Add labels and title
plt.xlabel('Date')
plt.ylabel('Values')
plt.title('Predictions vs. Observations For 1st Model')

# Add a legend with a better layout
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))

# Display grid lines
plt.grid(True, linestyle='--', alpha=0.7)

# Add data points to the legend for better identification
handles, labels = plt.gca().get_legend_handles_labels()
labels_and_handles = zip(labels, handles)
handles = [h[1] for h in sorted(labels_and_handles, key=lambda x: x[0])]
labels = [h[0] for h in sorted(labels_and_handles, key=lambda x: x[0])]
plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 1))

# Show the plot
plt.tight_layout()  # Adjust spacing
plt.show()

"""# ***SECOND MODEL***

**NOW INTRODUCING NEW FEATURES FOR OUR MODEL**
"""

#second_data=windowed_df.copy()
second_data = df_to_windowed_df(data, '2002-02-21', '2012-10-23', n=3)

second_data

pip install ta

second_data

#FEATURE ENGINEERING
import ta
print(second_data)
#1 . EMA
#second_data['EMA_8'] = second_data['Target'].ewm(span=8, adjust=False).mean()
#second_data['EMA_20'] = second_data['Target'].ewm(span=20, adjust=False).mean()
#second_data['EMA_50'] = second_data['Target'].ewm(span=50, adjust=False).mean()
#second_data['EMA_200'] = second_data['Target'].ewm(span=200, adjust=False).mean()

# A Moving Average is a trend-following indicator that smooths out price data over a specific period of time. It helps traders identify the overall direction of a price trend.
second_data['EMA_10'] = second_data['Target'].rolling(window=10).mean()  # 10-day moving average
second_data['EMA_20'] = second_data['Target'].rolling(window=20).mean()  # 20-day moving average
second_data['EMA_50'] = second_data['Target'].rolling(window=50).mean()  # 50-day moving average
second_data['EMA_200'] = second_data['Target'].rolling(window=200).mean()  # 200-day moving average


# 2. Volume Moving Average
#second_data['Volume_MA_10'] = second_data['Volume'].rolling(window=10).mean()  # 10-day volume moving average

# 3. RSI (Relative Strength Index)
#The RSI is a momentum oscillator that measures the speed and change of price movements. It helps traders identify overbought or oversold conditions in an asset.
second_data['RSI'] = ta.momentum.RSIIndicator(second_data['Target'], window=14).rsi()


 #MACD is a trend-following momentum indicator that shows the relationship between two moving averages of an asset's price. It helps traders identify potential trend reversals and the strength of a trend.
# 4. MACD (Moving Average Convergence Divergence)
macd = ta.trend.MACD(second_data['Target'])
second_data['MACD'] = ta.trend.macd_diff(second_data['Target'], window_slow=26, window_fast=12, window_sign=9)
second_data['MACD_Signal'] = ta.trend.macd_signal(second_data['Target'], window_slow=26, window_fast=12, window_sign=9)
#The MACD Signal Line, also known as the "signal line" or "trigger line," is a 9-period Exponential Moving Average (EMA) applied to the MACD line. It helps traders generate buy or sell signals based on the MACD indicator.
# Drop rows with NaN values created by lag features and moving averages
second_data = second_data.dropna()

# Display the DataFrame with new features
#second_data.head()

print(second_data)

second_data=second_data.reset_index(drop=True)
second_data['Close']=second_data['Target']
second_data=second_data.drop(['Target'],axis=1)
second_data.rename(columns={'Close': 'Target'}, inplace=True)
second_data

"""Implementing LIME"""

#!pip install lime

#from lime import lime_tabular
#import numpy as np

#def model_predict(data):
#     return model.predict(data)

# Creating a LIME Explainer
#explainer = lime_tabular.LimeTabularExplainer(
#    training_data=np.array(X_train), # Replace with your training data
#    feature_names=['Target1', 'Target2', 'Target3'], # Replace with your feature names
#    mode='regression'
#)

# Reshape the test instance to match the input shape expected by the model
#test_instance = X_test[0].reshape(1, -1)  # Adjust the reshape parameters as per your model's requirement

# Generating explanation for the selected instance
#explanation = explainer.explain_instance(
#    data_row=test_instance,
#    predict_fn=model_predict
#)

# Displaying the explanation
#explanation.show_in_notebook(show_table=True)

#second_data = second_data.drop(["Target-1","Target-2","Target-3","EMA_8","EMA_20","EMA_50","EMA_200"], axis=1)
#second_data = second_data.drop(["Target-1","Target-2","Target-3","MA_10","MA_50"], axis=1)
second_data = second_data.drop(["Target-1","Target-2","Target-3"], axis=1)
fb_data=second_data.copy()

"""Scalling"""

second_data

from sklearn.preprocessing import MinMaxScaler

# Assuming X_train1 is your training dataset and 'RSI' is the RSI feature
rsi_scaler = MinMaxScaler()
second_data['RSI'] = rsi_scaler.fit_transform(second_data[['RSI']])
second_data['MACD'] = rsi_scaler.fit_transform(second_data[['MACD']])
second_data['Volume'] = rsi_scaler.fit_transform(second_data[['MACD_Signal']])
second_data['Target'] = rsi_scaler.fit_transform(second_data[['Target']])
second_data['MACD_Signal'] = rsi_scaler.fit_transform(second_data[['MACD_Signal']])
second_data['EMA_10'] = rsi_scaler.fit_transform(second_data[['EMA_10']])
second_data['EMA_20'] = rsi_scaler.fit_transform(second_data[['EMA_20']])
second_data['EMA_50'] = rsi_scaler.fit_transform(second_data[['EMA_50']])
second_data['EMA_200'] = rsi_scaler.fit_transform(second_data[['EMA_200']])

# Plotting the technical indicators for 2017-2018 data
plt.figure(figsize=(12, 20))

# RSI plot for 2017-2018
plt.subplot(4, 1, 1)
plt.plot(second_data['Target Date'], second_data['RSI'], color='pink')
plt.plot(second_data['Target Date'], second_data['Target'], label='Closing Price', color='blue')
plt.title('Relative Strength Index (RSI)')
plt.axhline(.80, color='r', linestyle='dashed')
plt.axhline(.20, color='g', linestyle='dashed')
plt.ylabel('RSI')
plt.grid(True)

# MACD plot for 2017-2018
plt.subplot(4, 1, 2)
plt.plot(second_data['Target Date'], second_data['MACD'], label='MACD', color='blue')
plt.plot(second_data['Target Date'], second_data['MACD_Signal'], label='MACD_Signal', color='orange')
plt.plot(second_data['Target Date'], second_data['Target'], label='Closing Price', color='maroon')
plt.title('Moving Average Convergence Divergence (MACD) ')
plt.ylabel('MACD')
plt.legend()
plt.grid(True)


# Closing Price plot for 2017-2018
plt.subplot(4, 1, 4)
plt.plot(second_data['Target Date'], second_data['Target'], label='Closing Price', color='blue')
plt.plot(second_data['Target Date'], second_data['EMA_10'], label='EMA_10', color='pink')
plt.plot(second_data['Target Date'], second_data['EMA_20'], label='EMA_20', color='purple')
plt.plot(second_data['Target Date'], second_data['EMA_50'], label='EMA_50', color='yellow')
plt.plot(second_data['Target Date'], second_data['EMA_200'], label='EMA_200', color='green')

plt.title('S&P Closing Price')
plt.xlabel('Target Date')
plt.ylabel('Closing Price')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

"""Correlation"""

# Calculating the correlation matrix focusing on the correlation with 'Close'
correlation_matrix = second_data.corr().loc[:, ['Target']]
# Plotting the correlation heatmap
plt.figure(figsize=(8, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation of Features with Closing Price")
plt.show()

second_data.info()

"""Extracting "Date" , "Input" and "Output"
"""

#second_data['Close']=second_data['Target']
#second_data=second_data.drop(['Target'],axis=1)
print(second_data)
second_data.rename(columns={'Close': 'Target'}, inplace=True)

# Convert the DataFrame to a NumPy array
df_as_np1 = second_data.to_numpy()



# Extract dates from the first column
dates1 = df_as_np1[:,0]



# Extract the feature matrix from columns 1 to second-to-last column
middle_matrix1 = df_as_np1[:, 1:-1]

# Reshape the feature matrix
X1 = middle_matrix1.reshape((len(dates1), middle_matrix1.shape[1], 1))

# Extract the target values from the last column
Y1 = df_as_np1[:, -1]

# Convert arrays to np.float32 data type

X1 = X1.astype(np.float32)
y1 = Y1.astype(np.float32)




# Print the shapes of the arrays
print("Dates shape:", dates1.shape)
print("X1 shape:", X1.shape)
print("Y1 shape:", y1.shape)



"""Using random forrest for feature evalutaion"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
import numpy as np
from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()
y1_encoded = label_encoder.fit_transform(y1)


data = load_iris()
X = X1  # Features
y =y1_encoded  # Target labels


X = X.reshape(X.shape[0], -1)

# Create a Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model to the entire dataset
model.fit(X, y)

# Get feature importances
feature_importances = model.feature_importances_

# Rank features by importance
sorted_indices = np.argsort(feature_importances)[::-1]

# Print feature rankings
print("Feature Rankings:")
for rank, idx in enumerate(sorted_indices):
    print(f"Rank {rank + 1}: Feature {idx} - Importance: {feature_importances[idx]}")

# Select the top k features (e.g., top 2 features)
k = 2
selected_features = sorted_indices[:k]

# Use the selected features for further analysis or modeling
X_selected = X[:, selected_features]

"""Splitting"""

q_80 = int(len(dates1)* .85)
q_90 = int(len(dates1)* .9)

dates_train1, X_train1, y_train1 = dates1[:q_80],X1[:q_80],y1[:q_80]


dates_val1, X_val1, y_val1 = dates1[q_80:q_90],X1[q_80:q_90],y1[q_80:q_90]
dates_test1, X_test1, y_test1 = dates1[q_90:],X1[q_90:],y1[q_90:]

plt.plot(dates_train1, y_train1)
plt.plot(dates_val1, y_val1)
plt.plot(dates_test1, y_test1)

plt.legend(['Train', 'Validation', 'Test'])

"""GRIDSEARCH"""

from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score

def create_model1(layers,activation):
  model1=Sequential()
  for i,nodes in enumerate(layers):
    if i==0:
      model1.add(Dense(nodes,input_dim=X_train1.shape[1]))
      model1.add(Activation(activation))
    else:
      model1.add(Dense(nodes))
      model1.add(Activation(activation))
    model1.add(Dense(1))
    model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
    return model1

model1=KerasClassifier(build_fn=create_model1,verbose=0)

model1

layers=[[32],[40,20],[45,30,15],[64,32,32],[64,32],[20]]
activitations=['sigmoid','relu']
param_grid=dict(layers=layers,activation=activitations,batch_size=[128,256],epochs=[30])
grid1=GridSearchCV(estimator=model1,param_grid=param_grid)

#grid_result1=grid1.fit(X_train1,y_train1)

#[grid_result1.best_score_,grid_result1.best_params_]

"""TRAINING MODEL 2"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import layers


model1 = Sequential([layers.Input((8, 1)),
                    layers.LSTM(64),

                    layers.Dense(40, activation='sigmoid'),
                    layers.Dense(20, activation='sigmoid'),


                    layers.Dense(1)])

model1.compile(loss='mse',
              optimizer=Adam(learning_rate=0.001),
              metrics=['mean_absolute_error'])

model1.fit(X_train1, y_train1, validation_data=(X_val1, y_val1), epochs=30, batch_size=128)

train_predictions1 = model1.predict(X_train1).flatten()

plt.plot(dates_train1, train_predictions1)
plt.plot(dates_train1, y_train1)
plt.legend(['Training Predictions', 'Training Observations'])

val_predictions1 = model1.predict(X_val1).flatten()

plt.plot(dates_val1, val_predictions1)
plt.plot(dates_val1, y_val1)
plt.legend(['Validation Predictions', 'Validation Observations'])

test_predictions1 = model1.predict(X_test1).flatten()

plt.plot(dates_test1, test_predictions1)
plt.plot(dates_test1, y_test1)
plt.legend(['Testing Predictions', 'Testing Observations'])



"""**EVALUATION METRICS**

RMSE
"""

import numpy as np
from sklearn.metrics import mean_squared_error
rmse = np.sqrt(mean_squared_error(y_test1, test_predictions1))
print(rmse)

"""MAE"""

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test1, test_predictions1)
print(mae)

"""MSE"""

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test1, test_predictions1)
print(mse)

"""R2"""

from sklearn.metrics import r2_score
r2 = r2_score(y_test1, test_predictions1)
print(r2)

"""MAPE"""

def mean_absolute_percentage_error(y_true, y_pred1):
    return np.mean(np.abs((y_true - y_pred1) / y_true)) * 100

mape = mean_absolute_percentage_error(y_test1, test_predictions1)
print(mape)

"""The values of evaluation metrics may same perfect but our model have some fundamental issues as we have implemented the basic features using simplest of features

Prediction
"""

from copy import deepcopy

recursive_predictions1 = []
recursive_dates1 = np.concatenate([dates_val1, dates_test1])

last_window1 = deepcopy(X_train1[-1])  # Start with the last window from the training set

for target_date in recursive_dates1:
    # Predict the next value based on the current window
    next_prediction1 = model1.predict(np.array([last_window1])).flatten()
    recursive_predictions1.append(next_prediction1)


    # Shift the window: drop the first element and append the new prediction
    last_window1 = np.roll(last_window1, -1, axis=0)
    last_window1[-1] = next_prediction1

import matplotlib.pyplot as plt

# Define colors for each line
colors = {
    'Training Predictions': 'blue',
    'Training Observations': 'green',
    'Validation Predictions': 'orange',
    'Validation Observations': 'red',
    'Testing Predictions': 'purple',
    'Testing Observations': 'brown',
    'Recursive Predictions': 'magenta'
}

# Plot the data with labels and custom styles
plt.figure(figsize=(12, 6))  # Adjust the figure size

for label in colors.keys():
    plt.plot([], [], label=label, color=colors[label])  # Create empty lines for legend
n=10
# Plot the actual data
#plt.plot(dates_train1, train_predictions1, color=colors['Training Predictions'])
#plt.plot(dates_train1, y_train1, linestyle='--', color=colors['Training Observations'])
plt.plot(dates_val1[:n], val_predictions1[:n], color=colors['Validation Predictions'])
plt.plot(dates_val1[:n], y_val1[:n], linestyle='--', color=colors['Validation Observations'])
##plt.plot(dates_test1, test_predictions1, color=colors['Testing Predictions'])
#plt.plot(dates_test1, y_test1, linestyle='--', color=colors['Testing Observations'])
plt.plot(recursive_dates1[:n], recursive_predictions1[:n], color=colors['Recursive Predictions'])

# Add labels and title
plt.xlabel('Date')
plt.ylabel('Values')
plt.title('Predictions vs. Observations')

# Add a legend with a better layout
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))

# Display grid lines
plt.grid(True, linestyle='--', alpha=0.7)

# Add data points to the legend for better identification
handles, labels = plt.gca().get_legend_handles_labels()
labels_and_handles = zip(labels, handles)
handles = [h[1] for h in sorted(labels_and_handles, key=lambda x: x[0])]
labels = [h[0] for h in sorted(labels_and_handles, key=lambda x: x[0])]
plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 1))

# Show the plot
plt.tight_layout()  # Adjust spacing
plt.show()

""" KFOLD"""

from sklearn.model_selection import KFold
from tensorflow.keras.models import Sequential

layers = [64,20]  # Example: two hidden layers with 10 and 5 nodes
activation = 'sigmoid'  # Example activation function

# Define the KerasClassifier with these parameters
#model = KerasClassifier(build_fn=lambda: create_model(layers, activation), epochs=100, batch_size=10, verbose=0)

# Define the KerasClassifier with these parameters
#model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)

# Define K-Fold Cross-Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Specify the scoring metric (e.g., 'accuracy' for classification)
scoring_metric = 'accuracy'

# Perform K-Fold Cross-Validation with scoring
#results = cross_val_score(model1, X1, y1, cv=kfold, scoring=scoring_metric)

# Output the results
#print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

"""# FB PROPHET"""

fb_data

# Implementing Facebook Prophet for Time Series Forecasting
from prophet import Prophet


# Prepare the data for Prophet
df_prophet = fb_data.reset_index()[['Target Date', second_data.columns[-1]]]
# Include additional features in the original dataset
df_prophet.columns = ['ds', 'y']
#additional features
df_prophet['Volume'] = fb_data['Volume']
#df_prophet['EMA_10'] = fb_data['EMA_10']
#df_prophet['EMA_20'] = fb_data['EMA_20']
df_prophet['RSI'] = fb_data['RSI']
#df_prophet['EMA_50'] = fb_data['EMA_50']
df_prophet['EMA_200'] = fb_data['EMA_200']
df_prophet['Volume'] = fb_data['Volume']
df_prophet['MACD'] = fb_data['MACD']
df_prophet['MACD_Signal'] = fb_data['MACD_Signal']


model = Prophet()
model.add_regressor('Volume')
#model.add_regressor('EMA_10')
#model.add_regressor('EMA_20')
model.add_regressor('RSI')
#model.add_regressor('EMA_50')
#model.add_regressor('EMA_200')
model.add_regressor('MACD')
model.add_regressor('MACD_Signal')
model.fit(df_prophet)



# Create a future dataframe for predictions
future = model.make_future_dataframe(periods=365)  # Adjust periods as needed

# Set constant future values for additional features
future['Volume'] = fb_data['Volume'].mean()  # Replace with an appropriate constant value
#future['EMA_10'] = fb_data['EMA_10'].mean()  # Replace with an appropriate constant value
#future['EMA_20'] = fb_data['EMA_20'].mean()  # Replace with an appropriate constant value
future['RSI'] = fb_data['RSI'].mean()  # Replace with an appropriate constant value
#future['EMA_50'] = fb_data['EMA_50'].mean()  # Replace with an appropriate constant value
future['EMA_200'] = fb_data['EMA_200'].mean()  # Replace with an appropriate constant value
future['MACD'] = fb_data['MACD'].mean()  # Replace with an appropriate constant value
future['MACD_Signal'] = fb_data['MACD_Signal'].mean()  # Replace with an appropriate constant value

forecast1 = model.predict(future)

# Plotting the forecasts
import matplotlib.pyplot as plt

fig = model.plot(forecast1)
plt.title('Prophet Forecast')
plt.show()

# Plotting forecast components
fig2 = model.plot_components(forecast1)
plt.show()
import matplotlib.pyplot as plt

# Plotting the forecasts
fig = model.plot(forecast1)
plt.title('Prophet Forecast vs. Actual')
plt.xlabel('Date')
plt.ylabel('Target Variable (y)')

# Overlay the actual data on the plot
plt.plot(df_prophet['ds'], df_prophet['y'])
plt.legend(['Forecast', 'Actual'])

plt.show()

new_d['Date'] = pd.to_datetime(new_d['Date'], format='%Y-%m-%d')

# Filter rows for the specified date range
start_date = '2012-10-24'
end_date = '2013-10-24'

filtered_rows1 = new_d.loc[(new_d['Date'] >= start_date) & (new_d['Date'] <= end_date)].copy()

# Reset the index if needed
filtered_rows1.reset_index(drop=True, inplace=True)

# Display the filtered rows
print(filtered_rows1)

plt.plot(df_prophet['ds'], df_prophet['y'])
plt.plot(forecast1['ds'], forecast1['trend_upper'])
plt.plot(filtered_rows1['Date'], filtered_rows1['Close'])


plt.legend([ 'Actual','Forecast',"Feature value"])

plt.show()



forecast1['ds'] = pd.to_datetime(forecast1['ds'], format='%Y-%m-%d')
# Filter rows for the specified date range

start_date = '2012-10-24'
end_date = '2013-10-24'
predicted = forecast1.loc[(forecast1['ds'] >= start_date) & (forecast1['ds'] <= end_date)].copy()

# Reset the index if needed
predicted.reset_index(drop=True, inplace=True)

plt.plot(filtered_rows1['Date'], filtered_rows1['Close'])

plt.plot(predicted['ds'], predicted['trend_upper'])
plt.legend([ 'Actual','prediction'])

plt.show()

forecast1